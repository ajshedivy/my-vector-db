# Take-at-Home Task - Backend (Vector DB)

link: https://stack-ai.notion.site/Take-at-Home-Task-Backend-Vector-DB-bff06d35e031498fb6469875a40adeea
Congrats on making it thus far in the interview process!
Here is a task for you to show us where you shine the most üôÇ

The purpose is not to see how fast you go or what magic tricks you know in python, it‚Äôs mostly to understand how clearly you think and code. 

If you think clearly and your code is clean, you are better than 90% of applicants already!

> ‚ö†Ô∏è¬†Feel free to use Cursor, but use it where it makes sense, don‚Äôt overuse it, it introduces bugs and is super verbose and not really pythonic.
> 

## Objective

The goal of this project is to develop a REST API that allows users to **index** and **query** their documents within a Vector Database.

 A Vector Database specializes in storing and indexing vector embeddings, enabling fast retrieval and similarity searches. This capability is crucial for applications involving natural language processing, recommendation systems, and many more‚Ä¶

The REST API should be containerized in a Docker container.

### Definitions

To ensure a clear understanding, let's define some key concepts:

1. Chunk: A chunk is a piece of text with an associated embedding and metadata.
2. Document: A document is made out of multiple chunks, it also contains metadata.
3. Library: A library is made out of a list of documents and can also contain other metadata.

The API should:

1. Allow the users to create, read, update, and delete libraries.
2. Allow the users to create, read, update and delete documents and chunks within a library.
3. Index the contents of a library.
4. Do **k-Nearest Neighbor vector search** over the selected library with a given embedding query.

### Guidelines:

The code should be **Python** since that is what we use to develop our backend.

Here is a suggested path on how to implement a basic solution to the problem.

1. Define the Chunk, Document and Library classes. To simplify schema definition, we suggest you use a fixed schema for each of the classes. This means not letting the user define which fields should be present within the metadata for each class. 
    1. Following this path you will have fewer problems validating insertions/updates, but feel to let the users define their own schemas for each library if you are up for the challenge.
    2. Use **Pydantic** for these models.
2. Implement two or three indexing algorithms, do not use external libraries, we want to see you code them up. 
    1. What is the space and time complexity for each of the indexes? 
    2. Why did you choose this index?
3. Implement the necessary data structures/algorithms to ensure that there are no data races between reads and writes to the database. 
    1. Explain your design choices.
4. Create the logic to do the CRUD operations on libraries and documents/chunks. 
    1. Ideally use Services to decouple API endpoints from actual work 
5. Implement an API layer on top of that logic to let users interact with the vector database.
6. Create a docker image for the project

### Extra Points:

Here are some additional suggestions on how to enhance the project even further. You are not required to implement any of these, but if you do, we will value it. If you have other improvements in mind, please feel free to implement them and document them in the project‚Äôs README file

1. **Metadata filtering:**
    - Add the possibility of using metadata filters to enhance query results: ie: do kNN search over all chunks created after a given date, whose name contains xyz string etc etc.
2. **Persistence to Disk**:
    - Implement a mechanism to persist the database state to disk, ensuring that the docker container can be restarted and resume its operation from the last checkpoint. Explain your design choices and tradeoffs, considering factors like performance, consistency, and durability.
3. **Leader-Follower Architecture**:
    - Design and implement a leader-follower (master-slave) architecture to support multiple database nodes within the Kubernetes cluster. This architecture should handle read scalability and provide high availability. Explain how leader election, data replication, and failover are managed, along with the benefits and tradeoffs of this approach.
4. **Python SDK Client**:
    - Develop a Python SDK client that interfaces with your API, making it easier for users to interact with the vector database programmatically. Include  documentation and examples.
5. **Durable execution:**
    1. Use **Temporal (Python SDK)** to add **durable execution** to the *querying* process. You‚Äôll create a **Workflow** that orchestrates each step of handling a query, splitting the logic into separate **Activities**, and set up the local **worker** and **Temporal cluster** (using Docker auto-setup).
    - Workflow: The `QueryWorkflow`¬†that orchestrates the full query execution flow: User query ‚Üí preprocessing ‚Üí retrieval ‚Üí reranking ‚Üí answer generation
    - Client: You‚Äôll need a Temporal client that connects to the local Temporal server (from¬†`docker-auto-setup`) and can start workflows.
    - Worker: The worker runs separately and is responsible for¬†**executing activities and workflows**¬†on a specific task queue.
    - Temporal concepts to demonstrate:
        - Sync vs Async: Don‚Äôt mix async activities with blocking components.
        - Workflow vs Activity: Use the right patterns
        - Bonus: add signals/queries to the durable execution

## Constraints

Do **not** use libraries like chroma-db, pinecone, FAISS, etc to develop the project, we want to see you write the algorithms yourself. You can use numpy to calculate trigonometry functions `cos` , `sin` , etc

You **do not need to build a document processing pipeline** (ocr+text extraction+chunking) to test your system. Using a bunch of manually created chunks will suffice.

## **Tech Stack**

- **API Backend:** Python + FastAPI + Pydantic

## Resources:

[Cohere](https://cohere.com/embeddings) API key to create the embeddings for your test: try one of these

```jsx
pa6sRhnVAedMVClPAwoCvC1MjHKEwjtcGSTjWRMd
```

```markdown
rQsWxQJOK89Gp87QHo6qnGtPiWerGJOxvdg59o5f
```

## Evaluation Criteria

We will evaluate the code functionality and its quality.

**Code quality:**

- [SOLID design principles](https://realpython.com/solid-principles-python/).
- Use of static typing.
- FastAPI good practices.
- Pydantic schema validation
- Code modularity and reusability.
- Use of RESTful API endpoints.
- Project containerization with Docker.
- Testing
- Error handling.
- If you know what Domain-Driven design is, do it that way!
    - Separate API endpoints from business logic using services and from databases using repositories
- Keep code as pythonic as possible
- Do early returns
- Use inheritance where needed
- Use composition over inheritance
- Don‚Äôt hardcode values, for HTTP codes, [use this](https://fastapi.tiangolo.com/reference/status/).

**Functionality:**

- Does everything work as expected?

## Deliverable

1. **Source Code**: A link to a GitHub repository containing all your source code.
2. **Documentation**: A README file that documents the task, explains your technical choices, how to run the project locally, and any other relevant information.
3. **Demo video:**
    1. A screen recording where you show how to install the project and interact with it in real time.
    2. A screen recording of your design with an explanation of your design choices and thoughts/problem-solving.

## Timeline

As a reference, this task should take at most **4 days** (96h) from the receipt of this test to submit your deliverables üöÄ¬†

But honestly, if you think you can do a much better job with some extra days (perhaps because you couldn‚Äôt spend too many hours), be our guest! 

At the end of the day, if it is not going to impress the team, it‚Äôs not going to fly, so give it your best shot ‚úàÔ∏è

## Questions

Feel free to reach out at any given time with questions about the task, particularly if you encounter problems outside your control that may block your progress.