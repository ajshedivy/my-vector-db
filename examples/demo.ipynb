{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Vector Database - Demo\n",
    "\n",
    "This notebook demonstrates the complete functionality of the vector database and SDK.\n",
    "\n",
    "**Topics Covered**:\n",
    "1. Architecture Overview\n",
    "2. Creating Data\n",
    "3. Reading & Updating\n",
    "4. Vector Search\n",
    "5. Filtering\n",
    "6. Persistence\n",
    "7. Agno Integration\n",
    "8. Design Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Architecture Overview\n",
    "\n",
    "The vector database is organized in a **3-tier hierarchy**:\n",
    "- **Libraries**: Top-level containers with vector index configuration\n",
    "- **Documents**: Logical groupings within a library\n",
    "- **Chunks**: Individual searchable units with text, embeddings, and metadata\n",
    "\n",
    "### Key Design Principles\n",
    "\n",
    "1. **Layered Architecture**: Clean separation (API → Service → Storage → Index)\n",
    "2. **Thread-Safe**: RLock-based synchronization for concurrent operations\n",
    "3. **Type-Safe**: Full Pydantic validation throughout\n",
    "4. **Persistence**: JSON snapshots with atomic writes\n",
    "5. **Filtering**: Post-filtering strategy with declarative and custom options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection & Validation\n",
    "\n",
    "First, verify the API server is running and check initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_vector_db.sdk import VectorDBClient\n",
    "\n",
    "client = VectorDBClient(base_url=\"http://localhost:8000\")\n",
    "\n",
    "# Validate connection and check initial state\n",
    "status = client.get_persistence_status()\n",
    "print(f\"Connected to API\")\n",
    "print(f\"Persistence: {status['enabled']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Creating Data: Hierarchical Structure\n",
    "\n",
    "The hierarchical design allows flexible organization:\n",
    "- **Libraries** define index configuration (FLAT, HNSW) and distance metrics\n",
    "- **Documents** group related chunks (e.g., chapters in a book)\n",
    "- **Chunks** are the actual searchable units with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create library with index configuration\n",
    "library = client.create_library(\n",
    "    name=\"tech_articles\",\n",
    "    index_type=\"flat\",\n",
    "    index_config={\"metric\": \"cosine\"},\n",
    "    metadata={\"description\": \"Technology articles\", \"version\": \"1.0\"},\n",
    ")\n",
    "\n",
    "print(f\"Created library: {library.id}\")\n",
    "print(f\"Index type: {library.index_type}\")\n",
    "print(f\"Metric: {library.index_config['metric']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document within library\n",
    "document = client.create_document(\n",
    "    library_id=library.id,\n",
    "    name=\"tech_articles_2024\",\n",
    "    metadata={\"year\": 2024, \"source\": \"tech blogs\"},\n",
    ")\n",
    "\n",
    "print(f\"Created document: {document.id}\")\n",
    "print(f\"Name: {document.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Dataset\n",
    "\n",
    "This dataset includes:\n",
    "- **7 articles** across 3 categories (AI, Cloud, Security)\n",
    "- **Rich metadata**: category, topic, confidence scores, authors\n",
    "- **5D embeddings**: Simplified for demo purposes (production typically uses 768-1536 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [\n",
    "    # AI/ML articles\n",
    "    {\n",
    "        \"text\": \"Machine learning models use neural networks for pattern recognition\",\n",
    "        \"embedding\": [0.9, 0.8, 0.1, 0.2, 0.3],\n",
    "        \"metadata\": {\n",
    "            \"category\": \"ai\",\n",
    "            \"topic\": \"machine learning\",\n",
    "            \"confidence\": 0.95,\n",
    "            \"author\": \"Alice\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Deep learning architectures enable complex AI applications\",\n",
    "        \"embedding\": [0.85, 0.75, 0.15, 0.25, 0.35],\n",
    "        \"metadata\": {\n",
    "            \"category\": \"ai\",\n",
    "            \"topic\": \"deep learning\",\n",
    "            \"confidence\": 0.88,\n",
    "            \"author\": \"Bob\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Reinforcement learning enables autonomous decision making\",\n",
    "        \"embedding\": [0.87, 0.77, 0.13, 0.23, 0.33],\n",
    "        \"metadata\": {\n",
    "            \"category\": \"ai\",\n",
    "            \"topic\": \"reinforcement learning\",\n",
    "            \"confidence\": 0.90,\n",
    "            \"author\": \"Alice\",\n",
    "        },\n",
    "    },\n",
    "    # Cloud computing articles\n",
    "    {\n",
    "        \"text\": \"Cloud infrastructure provides scalable computing resources\",\n",
    "        \"embedding\": [0.3, 0.2, 0.9, 0.8, 0.1],\n",
    "        \"metadata\": {\n",
    "            \"category\": \"cloud\",\n",
    "            \"topic\": \"infrastructure\",\n",
    "            \"confidence\": 0.92,\n",
    "            \"author\": \"Charlie\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Kubernetes orchestrates containerized applications in the cloud\",\n",
    "        \"embedding\": [0.35, 0.25, 0.85, 0.75, 0.15],\n",
    "        \"metadata\": {\n",
    "            \"category\": \"cloud\",\n",
    "            \"topic\": \"kubernetes\",\n",
    "            \"confidence\": 0.89,\n",
    "            \"author\": \"Alice\",\n",
    "        },\n",
    "    },\n",
    "    # Security articles\n",
    "    {\n",
    "        \"text\": \"Cybersecurity best practices protect against data breaches\",\n",
    "        \"embedding\": [0.1, 0.2, 0.3, 0.9, 0.8],\n",
    "        \"metadata\": {\n",
    "            \"category\": \"security\",\n",
    "            \"topic\": \"cybersecurity\",\n",
    "            \"confidence\": 0.91,\n",
    "            \"author\": \"Bob\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Machine learning detects anomalies in network security\",\n",
    "        \"embedding\": [0.6, 0.5, 0.4, 0.7, 0.6],\n",
    "        \"metadata\": {\n",
    "            \"category\": \"security\",\n",
    "            \"topic\": \"machine learning\",\n",
    "            \"confidence\": 0.82,\n",
    "            \"author\": \"Charlie\",\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(articles)} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Insert\n",
    "\n",
    "**Design Pattern**: Batch operations are preferred over individual inserts.\n",
    "\n",
    "**Benefits**:\n",
    "- Single API call reduces HTTP round-trips\n",
    "- Atomic transactions (all-or-nothing)\n",
    "- Better performance for large datasets\n",
    "\n",
    "**Best Practice**: Always use `add_chunks()` for multiple inserts rather than looping with individual `create_chunk()` calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch insert all chunks at once\n",
    "chunks = client.add_chunks(document_id=document.id, chunks=articles)\n",
    "\n",
    "print(f\"Inserted {len(chunks)} chunks\")\n",
    "print(f\"Sample: {chunks[0].text[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Reading & Updating Data\n",
    "\n",
    "After creation, all entities can be accessed by their UUID without specifying parent relationships. This simplifies API usage while maintaining referential integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List operations at each level\n",
    "libraries = client.list_libraries()\n",
    "print(f\"Libraries: {len(libraries)}\")\n",
    "\n",
    "documents = client.list_documents(library_id=library.id)\n",
    "print(f\"Documents: {len(documents)}\")\n",
    "\n",
    "chunks = client.list_chunks(document_id=document.id)\n",
    "print(f\"Chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Operations\n",
    "\n",
    "Updates are performed on full objects. Notice how the `updated_at` timestamp changes while `created_at` remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update metadata on all chunks\n",
    "for chunk in client.list_chunks(document_id=document.id):\n",
    "    chunk.metadata[\"reviewed_by\"] = \"demo_bot\"\n",
    "    client.update_chunk(chunk)\n",
    "\n",
    "print(\"Updated all chunks with review metadata\")\n",
    "\n",
    "# Verify update\n",
    "sample = client.list_chunks(document_id=document.id)[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Vector Search\n",
    "\n",
    "The vector database performs **k-nearest neighbor (kNN)** search using the configured distance metric.\n",
    "\n",
    "**Current Implementation**: FLAT index\n",
    "- **Complexity**: O(n) - exhaustive search\n",
    "- **Recall**: 100% (exact search, guaranteed true nearest neighbors)\n",
    "- **Best for**: < 10,000 vectors\n",
    "\n",
    "**Alternative**: HNSW index (planned)\n",
    "- **Complexity**: O(log n) - approximate search\n",
    "- **Recall**: ~95-99% (tunable)\n",
    "- **Best for**: Millions of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query about AI/ML topics\n",
    "query_vector = [0.88, 0.78, 0.12, 0.22, 0.32]\n",
    "\n",
    "search_results = client.search(\n",
    "    library_id=library.id,\n",
    "    embedding=query_vector,\n",
    "    k=5,\n",
    ")\n",
    "\n",
    "print(f\"Query time: {search_results.query_time_ms:.2f}ms\")\n",
    "print(f\"Searched {len(search_results.results)} chunks\")\n",
    "print(f\"Index type: {library.index_type}\\n\")\n",
    "\n",
    "for i, result in enumerate(search_results.results, 1):\n",
    "    print(f\"{i}. Score: {result.score:.4f}\")\n",
    "    print(f\"   {result.text}\")\n",
    "    print(f\"   Category: {result.metadata['category']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Filtering: Two Approaches\n",
    "\n",
    "The SDK provides two complementary filtering strategies:\n",
    "\n",
    "### Approach 1: Declarative Filters (API-Compatible)\n",
    "- **JSON-serializable** filter definitions\n",
    "- **Server-side** filtering via REST API\n",
    "- Works with any HTTP client\n",
    "- **Use for**: Production deployments, cross-language clients\n",
    "\n",
    "### Approach 2: Custom Functions (SDK-Only)\n",
    "- **Python functions** for maximum flexibility\n",
    "- **Client-side** filtering with access to similarity scores\n",
    "- Complex logic without API changes\n",
    "- **Use for**: Prototyping, complex business logic, ad-hoc queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_vector_db.domain.models import (\n",
    "    SearchFilters,\n",
    "    FilterGroup,\n",
    "    MetadataFilter,\n",
    "    FilterOperator,\n",
    "    LogicalOperator,\n",
    ")\n",
    "\n",
    "# Simple metadata filter\n",
    "filters = SearchFilters(\n",
    "    metadata=FilterGroup(\n",
    "        operator=LogicalOperator.AND,\n",
    "        filters=[\n",
    "            MetadataFilter(field=\"category\", operator=FilterOperator.EQUALS, value=\"ai\")\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "results = client.search(\n",
    "    library_id=library.id,\n",
    "    embedding=query_vector,\n",
    "    k=5,\n",
    "    filters=filters,\n",
    ")\n",
    "\n",
    "print(\"Filtered by category='ai':\\n\")\n",
    "for i, result in enumerate(results.results, 1):\n",
    "    print(f\"{i}. {result.text[:60]}...\")\n",
    "    print(f\"   Category: {result.metadata['category']}\")\n",
    "    print(f\"   Score: {result.score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex AND/OR Filters\n",
    "\n",
    "Filters support nested logic for sophisticated queries. This example finds:\n",
    "- (AI articles with confidence > 0.9) **OR** (any security article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_filters = SearchFilters(\n",
    "    metadata=FilterGroup(\n",
    "        operator=LogicalOperator.OR,\n",
    "        filters=[\n",
    "            # High-confidence AI articles\n",
    "            FilterGroup(\n",
    "                operator=LogicalOperator.AND,\n",
    "                filters=[\n",
    "                    MetadataFilter(\n",
    "                        field=\"category\", operator=FilterOperator.EQUALS, value=\"ai\"\n",
    "                    ),\n",
    "                    MetadataFilter(\n",
    "                        field=\"confidence\",\n",
    "                        operator=FilterOperator.GREATER_THAN,\n",
    "                        value=0.9,\n",
    "                    ),\n",
    "                ],\n",
    "            ),\n",
    "            # OR any security article\n",
    "            MetadataFilter(\n",
    "                field=\"category\", operator=FilterOperator.EQUALS, value=\"security\"\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "results = client.search(\n",
    "    library_id=library.id,\n",
    "    embedding=query_vector,\n",
    "    k=10,\n",
    "    filters=complex_filters,\n",
    ")\n",
    "\n",
    "print(\"Filtered by (AI AND confidence>0.9) OR security:\\n\")\n",
    "for i, result in enumerate(results.results, 1):\n",
    "    print(f\"{i}. [{result.metadata['category']}] {result.text[:50]}...\")\n",
    "    print(\n",
    "        f\"   Confidence: {result.metadata['confidence']}, Score: {result.score:.4f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Filter Functions (SDK Only)\n",
    "\n",
    "For complex filtering logic, pass a Python function to `filter_function`. The function receives `SearchResult` objects (not `Chunk` objects), which include the similarity score.\n",
    "\n",
    "**Implementation Detail**: \n",
    "- Uses over-fetch strategy (k×3) to compensate for client-side filtering\n",
    "- Operates on `SearchResult` to avoid circular dependencies\n",
    "- Enables filtering based on similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_vector_db.sdk.models import SearchResult\n",
    "\n",
    "\n",
    "# Custom filter combining multiple conditions\n",
    "def high_quality_ai_by_alice(result: SearchResult) -> bool:\n",
    "    return (\n",
    "        result.score > 0.95\n",
    "        and result.metadata.get(\"category\") == \"ai\"\n",
    "        and result.metadata.get(\"author\") == \"Alice\"\n",
    "        and result.metadata.get(\"confidence\", 0) > 0.85\n",
    "    )\n",
    "\n",
    "\n",
    "results = client.search(\n",
    "    library_id=library.id,\n",
    "    embedding=query_vector,\n",
    "    k=5,\n",
    "    filter_function=high_quality_ai_by_alice,\n",
    ")\n",
    "\n",
    "print(\"Custom filter: high score, AI, Alice, high confidence\\n\")\n",
    "for i, result in enumerate(results.results, 1):\n",
    "    print(f\"{i}. {result.text}\")\n",
    "    print(\n",
    "        f\"   Score: {result.score:.4f}, Confidence: {result.metadata['confidence']}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda filter example - filter by text content\n",
    "results = client.search(\n",
    "    library_id=library.id,\n",
    "    embedding=query_vector,\n",
    "    k=5,\n",
    "    filter_function=lambda r: \"learning\" in r.text.lower(),\n",
    ")\n",
    "\n",
    "print(\"Lambda filter: contains 'learning'\\n\")\n",
    "for i, result in enumerate(results.results, 1):\n",
    "    print(f\"{i}. {result.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Persistence & Durability\n",
    "\n",
    "The database supports optional persistence with a simple snapshot-based approach.\n",
    "\n",
    "**Design Decision**: JSON snapshots\n",
    "- Saves entire state to JSON every N operations (default: 10)\n",
    "- Atomic writes using temp file + rename pattern\n",
    "- Human-readable format for debugging\n",
    "- Configurable via environment variables\n",
    "\n",
    "**Tradeoff**: \n",
    "- **Pro**: Simple implementation, easy debugging, no threading complexity\n",
    "- **Con**: May lose last N operations on crash\n",
    "- **Production Alternative**: PostgreSQL + pgvector with write-ahead logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Check current persistence status\n",
    "status = client.get_persistence_status()\n",
    "\n",
    "print(\"Persistence Status:\")\n",
    "print(json.dumps(status, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Restore Demo\n",
    "\n",
    "Demonstrate persistence by saving a snapshot, performing a destructive operation, then restoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save current state\n",
    "result = client.save_snapshot()\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the library (destructive operation)\n",
    "client.delete_library(library_id=library.id)\n",
    "\n",
    "libraries_after_delete = client.list_libraries()\n",
    "print(f\"After delete - Libraries: {len(libraries_after_delete)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore from snapshot\n",
    "result = client.restore_snapshot()\n",
    "\n",
    "# Verify restoration\n",
    "libraries_after_restore = client.list_libraries()\n",
    "print(f\"\\nVerification - Libraries: {len(libraries_after_restore)}\")\n",
    "for lib in libraries_after_restore:\n",
    "    print(f\"  {lib.name} ({lib.id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Real-World Integration: Agno RAG Agent\n",
    "\n",
    "This section demonstrates integration with the [Agno](https://github.com/agno-ai/agno) agent framework for building RAG (Retrieval-Augmented Generation) applications.\n",
    "\n",
    "**RAG Pattern**:\n",
    "1. User asks a question\n",
    "2. Agent searches vector DB for relevant context\n",
    "3. Context augments the LLM prompt\n",
    "4. LLM generates informed response\n",
    "\n",
    "**Why This Works**: The hierarchical design (libraries/documents/chunks) naturally maps to knowledge organization, making integration straightforward.\n",
    "\n",
    "See `examples/agno_example.py` for a complete working implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example integration pattern (see agno_example.py for full code)\n",
    "\n",
    "\n",
    "from agno.agent import Agent\n",
    "from agno.knowledge.knowledge import Knowledge\n",
    "from agno.models.anthropic import Claude\n",
    "from my_vector_db.db import MyVectorDB\n",
    "\n",
    "# Create vector database connection\n",
    "vector_db = MyVectorDB(\n",
    "    api_base_url=\"http://localhost:8000\",\n",
    "    library_name=\"Python Programming Guide\",\n",
    "    index_type=\"flat\",\n",
    ")\n",
    "\n",
    "# Create knowledge base that uses our vector DB\n",
    "knowledge = Knowledge(name=\"Tech Knowledge Base\", vector_db=vector_db, max_results=5)\n",
    "\n",
    "# Create agent with RAG capabilities\n",
    "agent = Agent(\n",
    "    name=\"Tech Assistant\",\n",
    "    knowledge=knowledge,\n",
    "    model=Claude(id=\"claude-sonnet-4-5\"),\n",
    "    search_knowledge=True,  # Enable RAG\n",
    ")\n",
    "\n",
    "# Start interactive CLI (agent searches vector DB automatically)\n",
    "agent.print_response(\n",
    "    \"what are the latest trends in AI and cloud computing?\", stream=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Design Patterns & Best Practices\n",
    "\n",
    "This section summarizes key design decisions and recommended practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Batch Operations Pattern\n",
    "\n",
    "**Recommended**:\n",
    "```python\n",
    "chunks = client.add_chunks(document_id=doc.id, chunks=large_list)\n",
    "```\n",
    "\n",
    "**Avoid**:\n",
    "```python\n",
    "for chunk in large_list:\n",
    "    client.create_chunk(...)  # Many HTTP round-trips\n",
    "```\n",
    "\n",
    "**Takeaway**: Batch operations reduce network overhead and enable atomic transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Index Selection: FLAT vs HNSW\n",
    "\n",
    "| Metric | Flat Index | HNSW Index |\n",
    "|--------|------------|------------|\n",
    "| Search | O(n) - exact | O(log n) - approximate |\n",
    "| Insert | O(1) | O(log n) |\n",
    "| Recall | 100% | 95-99% (tunable) |\n",
    "| Best For | <10K vectors | Millions of vectors |\n",
    "\n",
    "**Recommendation**: Start with FLAT for accuracy and simplicity. Migrate to HNSW when dataset grows beyond 10,000 vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Post-Filtering Strategy\n",
    "\n",
    "**Algorithm**:\n",
    "1. Perform kNN vector search → get candidates\n",
    "2. Fetch full chunk data from storage\n",
    "3. Apply metadata filters\n",
    "4. Return top k results\n",
    "\n",
    "**Tradeoff**: \n",
    "- **Pro**: Simple implementation, works with any index type\n",
    "- **Pro**: Index layer doesn't need filter logic\n",
    "- **Con**: May not return k results if filters are highly selective\n",
    "- **Con**: Requires over-fetching (k×3 for custom filters)\n",
    "\n",
    "**Production Alternative**: Pre-filtering with bitmap indexes for highly selective queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Type Safety Throughout\n",
    "\n",
    "**Pattern**: Pydantic models everywhere\n",
    "```python\n",
    "library: Library = client.create_library(...)  # Type-checked\n",
    "document: Document = client.create_document(...)  # Validated\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- Runtime validation catches errors early\n",
    "- IDE autocomplete improves developer experience\n",
    "- Auto-generated OpenAPI documentation\n",
    "- Prevents entire classes of bugs\n",
    "\n",
    "**Tradeoff**: Slight performance overhead (~10-15%), but worth it for reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Layered Architecture\n",
    "\n",
    "**Layers**: API → Service → Storage → Index\n",
    "\n",
    "**Benefits**:\n",
    "- **Separation of concerns**: Each layer has clear responsibilities\n",
    "- **Testable**: Can test each layer in isolation\n",
    "- **Extensible**: Can swap implementations without affecting other layers\n",
    "\n",
    "**Example**: Adding HNSW index requires only implementing the index interface. API and Service layers remain unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Pragmatic Design Choices\n",
    "\n",
    "| Feature | Current | Production Alternative |\n",
    "|---------|---------|------------------------|\n",
    "| Persistence | JSON snapshots | PostgreSQL + WAL |\n",
    "| Filtering | Post-filtering | Pre-filtering with bitmaps |\n",
    "| Index | FLAT (O(n)) | HNSW (O(log n)) |\n",
    "| Locking | Coarse RLock | Fine-grained locks |\n",
    "| Storage | In-memory | Distributed (Redis, etc.) |\n",
    "\n",
    "**Philosophy**: Start simple, scale where needed.\n",
    "\n",
    "The current design is:\n",
    "- Easy to understand and debug\n",
    "- Production-ready for moderate scale (<100K vectors)\n",
    "- Extensible for larger scale with clear upgrade paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close client connection\n",
    "client.delete_library(library_id=library.id)  # Clean up\n",
    "client.close()\n",
    "print(\"Client connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **API Documentation**: http://localhost:8000/docs\n",
    "2. **SDK Reference**: `docs/README.md`\n",
    "3. **More Examples**: `examples/` directory\n",
    "4. **Run Tests**: `uv run pytest`\n",
    "\n",
    "**Discussion Questions**:\n",
    "- When would you switch from FLAT to HNSW index?\n",
    "- How would you implement distributed storage?\n",
    "- What monitoring and observability would you add?\n",
    "- How would you handle schema migrations?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
